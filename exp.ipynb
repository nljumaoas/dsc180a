{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer: 164167680\n",
      "RMS Norm Layer: 1280\n",
      "Query (Q): 1638400\n",
      "Key (K): 1638400.0\n",
      "Value (V): 1638400.0\n",
      "Output (O): 1638400\n",
      "MLP: 17203200\n",
      "Attention Blocks: 285112320.0\n",
      "All Layers: 449281280.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "449281280.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_layers(V, H, I, N, h=None, g_size=None):\n",
    "    # Embedding Layer\n",
    "    embedding_layer = V * H\n",
    "    print(f\"Embedding Layer: {embedding_layer}\")\n",
    "\n",
    "    # RMS Norm Layer\n",
    "    rms_norm_layer = H\n",
    "    print(f\"RMS Norm Layer: {rms_norm_layer}\")\n",
    "\n",
    "    # Query (Q), Key (K), Value (V)\n",
    "    q = H * H  # Or can be H * h * (H / h), but it simplifies to H * H\n",
    "    if g_size is None:\n",
    "        k = H * H\n",
    "        v = H * H\n",
    "    else:\n",
    "        k = H * H / g_size\n",
    "        v = H * H / g_size\n",
    "    print(f\"Query (Q): {q}\")\n",
    "    print(f\"Key (K): {k}\")\n",
    "    print(f\"Value (V): {v}\")\n",
    "\n",
    "    # Output (O)\n",
    "    o = H * H\n",
    "    print(f\"Output (O): {o}\")\n",
    "\n",
    "    # MLP calculation\n",
    "    mlp = 2 * H * I + I * H\n",
    "    print(f\"MLP: {mlp}\")\n",
    "\n",
    "    # Attention Blocks\n",
    "    attention_blocks = N * (2 * rms_norm_layer + q + k + v + o + mlp)\n",
    "    print(f\"Attention Blocks: {attention_blocks}\")\n",
    "\n",
    "    # All Layers (Sum of all components)\n",
    "    all_layers = embedding_layer + attention_blocks + rms_norm_layer \n",
    "\n",
    "    print(f\"All Layers: {all_layers}\")\n",
    "    return all_layers\n",
    "\n",
    "calculate_layers(128256, 2048, 4480, 12, g_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MFU: 31.9310%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "config_path = './configs/model_configs/llama_750M_config.json'\n",
    "def MFU_calculation_with_config(config_path, batch_size, sequence_length, number_of_GPU, GPU_peak_TFLOPS, iteration_time):\n",
    "    \"\"\"\n",
    "    Calculates Model FLOPs Utilization (MFU) for a given model and hardware setup, using a configuration file.\n",
    "\n",
    "    Parameters:\n",
    "    - config_path (str): Path to the model's configuration JSON file.\n",
    "    - batch_size (int): Batch size used in training.\n",
    "    - sequence_length (int): Sequence length of the input data.\n",
    "    - number_of_GPU (int): Number of GPUs used.\n",
    "    - GPU_peak_TFLOPS (float): Theoretical peak TFLOPs of a single GPU.\n",
    "    - iteration_time (float): Time taken for one iteration (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    - str: MFU as a percentage rounded to 4 decimal places.\n",
    "    \"\"\"\n",
    "    # Load configuration from the JSON file\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    \n",
    "    v = config.get('vocab_size', 0)  # Vocabulary size\n",
    "    n = config.get('num_attention_heads', 0)  # Number of attention heads\n",
    "    h = config.get('hidden_size', 0)  # Hidden state dimension\n",
    "    i = config.get('intermediate_size', 0)  # SwiGLU projection dimension\n",
    "    N = config.get('num_hidden_layers', 0)  # Number of layers\n",
    "    \n",
    "    # Ensure all parameters are available\n",
    "    if not all([v, n, h, i, N]):\n",
    "        raise ValueError(\"Configuration file is missing one or more required parameters.\")\n",
    "    \n",
    "    b = batch_size\n",
    "    s = sequence_length\n",
    "\n",
    "    # FLOPs calculation for one forward pass\n",
    "    flops_per_forward = (\n",
    "        N * (6 * b * s * h**2 + 4 * b * s**2 * h + 3 * b * s**2 * n + 2 * b * s * h**2)\n",
    "        + N * (6 * b * s * h * i)\n",
    "        + 2 * b * s * h * v\n",
    "    )\n",
    "\n",
    "    # Forward-backward pass is roughly 3 times the forward pass FLOPs\n",
    "    flops_per_forward_backward = (3 * flops_per_forward) / 10**12  # Convert to TFLOPs\n",
    "\n",
    "    # GPU peak TFLOPs per iteration\n",
    "    GPU_TFLOPs_per_iteration = number_of_GPU * GPU_peak_TFLOPS * iteration_time\n",
    "\n",
    "    # Calculate MFU\n",
    "    MFU = (flops_per_forward_backward / GPU_TFLOPs_per_iteration) * 100\n",
    "\n",
    "    # Return MFU rounded to 4 decimal places\n",
    "    return f\"MFU: {MFU:.4f}%\"\n",
    "\n",
    "MFU_calculation_with_config(config_path=config_path, batch_size=240, sequence_length=1024, number_of_GPU=4, GPU_peak_TFLOPS=149.7, iteration_time=6.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilized FLOPs (C): 2.48e+19\n",
      "N_opt(C): 320.5662505403389 Million\n",
      "D_opt(C): 5.51781946390067 Billion\n",
      "L_opt(C): 2.80e+00\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "gpu_flops = 149.7 * 10**12  # GPU FLOPs in teraflops\n",
    "day_time = 24 * 60 * 60  * 2   # Seconds in a day\n",
    "num_gpus = 4              # Number of GPUs\n",
    "mfu = 0.24          # Model utilization factor\n",
    "\n",
    "# Calculate utilized FLOPs (C)\n",
    "C = gpu_flops * day_time * num_gpus * mfu\n",
    "print(f\"Utilized FLOPs (C): {C:.2e}\")\n",
    "\n",
    "# Define functions for N_opt(C), D_opt(C), and L_opt(C)\n",
    "def N_opt(C):\n",
    "    \"\"\"Calculates optimal number of parameters (N_opt) given utilized FLOPs (C).\"\"\"\n",
    "    return 0.6 * C**0.45\n",
    "\n",
    "def D_opt(C):\n",
    "    \"\"\"Calculates optimal dataset size (D_opt) given utilized FLOPs (C).\"\"\"\n",
    "    return 0.29 * C**0.53\n",
    "\n",
    "def L_opt(C):\n",
    "    \"\"\"Calculates optimal loss (L_opt) given utilized FLOPs (C).\"\"\"\n",
    "    return 1070 * C**-0.154 + 1.7\n",
    "\n",
    "# Calculate values for N_opt, D_opt, and L_opt\n",
    "N_opt_value = N_opt(C)\n",
    "D_opt_value = D_opt(C)\n",
    "L_opt_value = L_opt(C)\n",
    "\n",
    "# Print results\n",
    "print(f\"N_opt(C): {N_opt_value/ 10**6} Million\")\n",
    "print(f\"D_opt(C): {D_opt_value / 10**9} Billion\")\n",
    "print(f\"L_opt(C): {L_opt_value:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750.0980463529183"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C / (6 * D_opt_value) / 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40462427745664736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7/17.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
